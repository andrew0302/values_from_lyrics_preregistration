<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.324">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrew M. Demetriou">

<title>Towards Automated Estimation of Psychological Values from Song Lyrics: Pre-Registration</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="pre-registration_html_files/libs/clipboard/clipboard.min.js"></script>
<script src="pre-registration_html_files/libs/quarto-html/quarto.js"></script>
<script src="pre-registration_html_files/libs/quarto-html/popper.min.js"></script>
<script src="pre-registration_html_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="pre-registration_html_files/libs/quarto-html/anchor.min.js"></script>
<link href="pre-registration_html_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="pre-registration_html_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="pre-registration_html_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="pre-registration_html_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="pre-registration_html_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="pre-registration_html_files/libs/quarto-diagram/mermaid.min.js"></script>
<script src="pre-registration_html_files/libs/quarto-diagram/mermaid-init.js"></script>
<link href="pre-registration_html_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">

<script src="pre-registration_html_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="pre-registration_html_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#study-information" id="toc-study-information" class="nav-link active" data-scroll-target="#study-information">Study Information</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#sec-hypotheses" id="toc-sec-hypotheses" class="nav-link" data-scroll-target="#sec-hypotheses">Hypotheses</a></li>
  <li><a href="#study-type" id="toc-study-type" class="nav-link" data-scroll-target="#study-type">Study Type</a></li>
  <li><a href="#blinding" id="toc-blinding" class="nav-link" data-scroll-target="#blinding">Blinding</a></li>
  </ul></li>
  <li><a href="#design-plan" id="toc-design-plan" class="nav-link" data-scroll-target="#design-plan">Design Plan</a>
  <ul class="collapse">
  <li><a href="#participant-recruitment-platform" id="toc-participant-recruitment-platform" class="nav-link" data-scroll-target="#participant-recruitment-platform">Participant Recruitment Platform</a></li>
  <li><a href="#survey-platform" id="toc-survey-platform" class="nav-link" data-scroll-target="#survey-platform">Survey Platform</a></li>
  <li><a href="#survey-measures" id="toc-survey-measures" class="nav-link" data-scroll-target="#survey-measures">Survey Measures</a></li>
  <li><a href="#sec-sampling" id="toc-sec-sampling" class="nav-link" data-scroll-target="#sec-sampling">Sampling Procedures</a></li>
  <li><a href="#natural-language-processing" id="toc-natural-language-processing" class="nav-link" data-scroll-target="#natural-language-processing">Natural Language Processing</a></li>
  <li><a href="#analytic-approach" id="toc-analytic-approach" class="nav-link" data-scroll-target="#analytic-approach">Analytic approach</a></li>
  <li><a href="#contributions" id="toc-contributions" class="nav-link" data-scroll-target="#contributions">Contributions</a></li>
  </ul></li>
  
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Towards Automated Estimation of Psychological Values from Song Lyrics: Pre-Registration</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Andrew M. Demetriou </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="study-information" class="level2">
<h2 class="anchored" data-anchor-id="study-information">Study Information</h2>
<p><strong>Contributors</strong>: Andrew M. Demetriou, Jaehun Kim, Sandy Manolios, Cynthia C.S. Liem</p>
<p><font size="2"> See <a href="#sec-contributions">Section&nbsp;1.7</a> for details of individual contributions according to the <a href="https://www.kent.ac.uk/guides/credit-contributor-roles-taxonomy#:~:text=CRediT%20(Contributor%20Roles%20Taxonomy)%20is,contribution%20to%20the%20scholarly%20output.">Contributor Roles Taxonomy</a>.<br>
</font></p>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>We aim to extend work that used natural language processing (NLP) to estimate psychological values (e.g. <span class="citation" data-cites="schwartz_extending_2001">(<a href="#ref-schwartz_extending_2001" role="doc-biblioref">Schwartz et al. 2001</a>)</span>) in social-media text <span class="citation" data-cites="ponizovskiy_development_2020">(<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">Ponizovskiy et al. 2020</a>)</span>. Specifically we will explore the potential to estimate the perceived psychological values in song lyrics.</p>
<p>Our study primarily consists of comparisons between what people indicate, and what NLP systems estimate, are the psychological values expressed in song lyrics. We will gather responses from people using an online survey. We will also use NLP systems to estimate the scores and compare them to those from people.</p>
<p>As in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span> we estimate convergent validity of the grouped NLP systems and our queztionnaire by estimating correlations with related constructs measured using Linguistic Inquiry Word Count (<a href="https://liwc.app">LIWC</a>; <span class="citation" data-cites="boyd_development_2022">Boyd et al. (<a href="#ref-boyd_development_2022" role="doc-biblioref">2022</a>)</span>). We will also compare the performance of word counting methods used in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span> with semantic distance methods similar to those used in e.g. <span class="citation" data-cites="beaty_automating_2021">Beaty and Johnson (<a href="#ref-beaty_automating_2021" role="doc-biblioref">2021</a>)</span>.</p>
<p>People who are exceptionally familiar or who have a strong preference for song lyrics may give different ratings than others. To examine this in the future, we also begin development of a psychometric questionnaire aimed at measuring lyric preference intensity and expertise.</p>
<p>Lastly, we aim to make artifacts of this project reproducible. Thus, reusable instances of the NLP systems will be available on <a href="https://replicate.com/eldrin/text-concept-similarity">Replicate</a>, codebases and notebooks documenting our work will be available on <a href="https://https://github.com/andrew0302/values_from_lyrics">Github</a>, as well as <a href="https://osf.io/h87mz/">Open Science Framework</a> which will also include de-identified responses from participants. These repositories will also contain the files we used to make our <a href="https://formR.org">formR.org</a> survey, a test version of which can be found <a href="https://testmysurvey.formr.org/">here</a>.</p>
</section>
<section id="sec-hypotheses" class="level3">
<h3 class="anchored" data-anchor-id="sec-hypotheses">Hypotheses</h3>
<p>As this is an initial study, our hypotheses are not severe:</p>
<p><strong>Primary Hypothesis</strong>: Grouped NLP systems show a statistically significant correlation with grouped <strong>a)</strong> participant ratings across all 10 personal values, and <strong>b)</strong> with related LIWC constructs - in the same or greater magnitude as shown in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span>.</p>
<p><strong>Primary Null Hypothesis</strong>: Grouped NLP systems show no evidence of a correlation with participant ratings across all 10 personal values, or LIWC constructs.</p>
<p><strong>Secondary Hypothesis</strong>: Magnitude of these correlations will be lower with word-counting methods than with semantic distance methods.</p>
<p><strong>Secondary Null Hypothesis</strong>: Magnitude of these correlations show no difference.</p>
</section>
<section id="study-type" class="level3">
<h3 class="anchored" data-anchor-id="study-type">Study Type</h3>
<p>Observational study - Data is collected from study participants that are not randomly assigned to a treatment.</p>
</section>
<section id="blinding" class="level3">
<h3 class="anchored" data-anchor-id="blinding">Blinding</h3>
<p>No blinding is involved in this study</p>
</section>
</section>
<section id="design-plan" class="level2">
<h2 class="anchored" data-anchor-id="design-plan">Design Plan</h2>
<section id="participant-recruitment-platform" class="level3">
<h3 class="anchored" data-anchor-id="participant-recruitment-platform">Participant Recruitment Platform</h3>
<p>We will recruit a U.S. representative sample of participants from <a href="https://prolific.co">Prolific.co</a>.</p>
</section>
<section id="survey-platform" class="level3">
<h3 class="anchored" data-anchor-id="survey-platform">Survey Platform</h3>
<p>Our primary measure is the perceived presence of personal values in song lyrics. Song lyrics may be written from the perspective of the author, but also from the perspective of someone or something else - sometimes referred to as the ‘speaker’. As we are measuring the presence of values as suggested in the lyrics themselves, we explicitly ask participants to respond with the perspective of the <em>speaker</em> in mind, and not the author.</p>
<p>The survey will be implemented on an instance of <a href="https://formR.org">formR.org</a> hosted on the servers of <a href="https://www.tudelft.nl/">Delft University of Technology</a> to ensure GDPR compliance. A test version of the survey can be found <a href="https://testmysurvey.formr.org/">here</a>. The <code>.csv</code> survey files used as input to formR were constructed in R<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The main component of the survey involves showing participants the lyrics to a number of songs, one at a time. For each song they are asked to respond to set of questions designed to assess the presence of values in the lyrics.</p>
<p>The majority of items require a Likert-type response. In order to gather a more continuous measure, we used a slider with no starting point: the <code>rating button</code> option in formR shows a horizontal gray bar with two labeled poles (e.g.&nbsp;agree - disagree). For each value there is a brief description, followed by the slider labelled (“opposed to their values” - “of supreme importance”).</p>
<p><img src="screenshots/formR.png" class="img-fluid"></p>
<p>Participants are instructed to indicate on the bar the degree to which they e.g.&nbsp;agree or disagree, as they might with a slider. However, the gray bar has no visible slider, thus no starting value. Although it shows no divisions it contains 20 subdivisions. To ensure that participants understand the use of this method, we include a ‘training and explanation’ page at the beginning of our <a href="https://testmysurvey.formr.org/">survey</a>.</p>
<section id="randomization" class="level4">
<h4 class="anchored" data-anchor-id="randomization">Randomization</h4>
<p>Participants are randomly assigned 18 lyric stimuli to rate. We experienced issues with the formR platform when the number of lyric stimuli in the survey was greater than 60. Thus, our stimulus set will be separated into otherwise identical survey files on the formR server, with no more than 60 lyrics in each survey file. Participants will be randomly assigned to one of the surveys, which will in turn randomly select a subset of stimuli to be rated.</p>
</section>
</section>
<section id="survey-measures" class="level3">
<h3 class="anchored" data-anchor-id="survey-measures">Survey Measures</h3>
<section id="personal-values" class="level4">
<h4 class="anchored" data-anchor-id="personal-values">Personal Values</h4>
<p>Prior research (e.g. <span class="citation" data-cites="schwartz_extending_2001">Schwartz et al. (<a href="#ref-schwartz_extending_2001" role="doc-biblioref">2001</a>)</span>) has shown evidence for the presence of personal values as guiding principles in the lives of people. Participants will indicate the degree to which they think 10 values are present for each set of lyrics that they are shown. We chose to use the Short Schwarz’s Value Survey <span class="citation" data-cites="lindeman_measuring_2005">Lindeman and Verkasalo (<a href="#ref-lindeman_measuring_2005" role="doc-biblioref">2005</a>)</span> as it is the briefest instrument whose reliability and validity has been shown to be adequate, to our knowledge. The original instrument displays a brief definition of each of the ten values in the Schwartz inventory, (e.g.&nbsp;“POWER (social power, authority, wealth)”) and asks participants to indicate on a Likert scale (0= Opposed to my principles, 8 = Of supreme importance) the degree of importance of the value to them. In our version, participants will indicate on a solid gray bar as described above. As our participants will be rating a stimulus that is not themselves, we adjusted the wording slightly: e.g.&nbsp;“Please, rate the importance of the following values as a life-guiding principle for the SPEAKER of the lyrics.”</p>
</section>
<section id="lyric-preferences" class="level4">
<h4 class="anchored" data-anchor-id="lyric-preferences">Lyric Preferences</h4>
<p>To assess whether expertise in lyrics or a preference for lyrical content has an effect on the ratings given, we have begun developing a scale, partially inspired by the Preference Intensity scale in <span class="citation" data-cites="schafer_functions_2009">Schäfer and Sedlmeier (<a href="#ref-schafer_functions_2009" role="doc-biblioref">2009</a>)</span>. Our original ad-hoc scale consisted of 10 Likert-type items. Participants in our second pilot (see <a href="#sec-pilot2">Section&nbsp;1.2.2</a>) were asked to respond to the 10 items, and to an additional ‘open response’ format item that asked: “Can you think of any other activities or indications that someone has an affinity for song lyrics? If so, please enter them here:”. We removed poorly performing items, and added 5 items based on participant responses to the open format question. As this instrument has yet to show satisfactory reliability or validity, we will continue adding and removing items using factor analysis and item response theory techniques as we progress.</p>
</section>
<section id="additional-measures" class="level4">
<h4 class="anchored" data-anchor-id="additional-measures">Additional Measures</h4>
<section id="familiarity" class="level5">
<h5 class="anchored" data-anchor-id="familiarity">Familiarity</h5>
<p>To control for familiarity of the lyrics, we will ask participants to indicate (yes/no) if they recognize the song that the lyrics came from. In addition, we will ask the participants whether or not they think the speaker and writer are the same person as an exploratory measure.</p>
</section>
<section id="rating-confidence" class="level5">
<h5 class="anchored" data-anchor-id="rating-confidence">Rating Confidence</h5>
<p>It has been suggested that a rater’s confidence in their annotation is a relevant indicator of reliability (although possibly orthogonal to accuracy; see <span class="citation" data-cites="cabitza_as_2020">Cabitza, Campagner, and Sconfienza (<a href="#ref-cabitza_as_2020" role="doc-biblioref">2020</a>)</span>). For each set of lyrics, we will ask participants to indicate the degree to which they are confident in their ratings on a solid bar ranging from ‘Extremely unconfident’ to ‘Extremely confident’).</p>
<p>Our pilot study (<a href="#sec-pilot2">Section&nbsp;1.2.2</a>) of 20 lyric stimuli suggests participants are overall ‘Somewhat confident’ in their responses, which provides some initial evidence of self-perceived intra-rater reliability of our procedure.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="pre-registration_html_files/figure-html/fig-1-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Participant self-ratings of Confidence in own responses for 20 lyric stimuli</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-sampling" class="level3">
<h3 class="anchored" data-anchor-id="sec-sampling">Sampling Procedures</h3>
<section id="lyric-stimulus-set" class="level4">
<h4 class="anchored" data-anchor-id="lyric-stimulus-set">Lyric Stimulus Set</h4>
<p>We aim to annotate a total of 360 lyric stimuli drawn from a pool of 2200, with approximately 25 ratings for each. This is not intended to be a fully representative sample, but rather a sufficiently large sample with which to examine the potential of our procedure. Size limits were determined by estimating the smallest sample size to demonstrate the viability of our procedure, taking into account time and budgetary constraints of the research team.</p>
<p>Our 360 stimulus set was derived using stratified random sampling (see <a href="#sec-stratifiedsampling">Section&nbsp;1.4</a> for details), and then a final manual screening by the research team (see <a href="#sec-manualscreening">Section&nbsp;1.5</a>).</p>
<p>The overall population of song lyrics was derived from the Spotify Million Playlist Dataset <a href="https://aicrowd.com/challenges/spotify-million-playlist-dataset-challenge">MPD</a> which contains 1 million Spotify user-generated playlists, chosen because of its size and its recency vs.&nbsp;other similar datasets.</p>
<p>The lyric stimuli were drawn from the database of <a href="https://musixmatch.com">musiXmatch</a> using their API, which provides approximately 30% of the lyrics of each song.</p>
</section>
<section id="number-of-ratings" class="level4">
<h4 class="anchored" data-anchor-id="number-of-ratings">Number of ratings</h4>
<section id="task-subjectivity" class="level5">
<h5 class="anchored" data-anchor-id="task-subjectivity">Task Subjectivity</h5>
<p>It may be the case that the number of raters required to reach a satisfactory inter-rater reliability increases with the degree to which the task is subjective. Thus, at the very end of the survey we will ask participants to indicate the degree to which they found the task to be subjective on a solid bar ranging from ‘Completely subjective’ to ‘Completely objective’). The mode of responses in our pilot study (see <a href="#sec-pilot2">Section&nbsp;1.2.2</a>) suggests participants find the task to be ‘Very Subjective’. Thus we expect we will need a relatively large number of ratings per lyric stimulus for our study.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="pre-registration_html_files/figure-html/fig-2-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> Participant ratings of subjectivity of the lyric rating task.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We followed a procedure inspired by <span class="citation" data-cites="debruine_determining_2018">DeBruine and Jones (<a href="#ref-debruine_determining_2018" role="doc-biblioref">2018</a>)</span> to determine the number of ratings per lyric stimulus. Specifically, we chose a small subset of 20 lyric samples, and had approximately 500 participants rate them all. We then randomly sub-sampled from the pool of 500 raters in increments ranging from 5 to 50 raters, and estimated Cronbach’s alpha for each subsample. Our conclusions suggested conservative estimates of 25 raters per lyric stimulus. See <a href="#sec-pilot2">Section&nbsp;1.2.2</a> for details.</p>
</section>
</section>
<section id="number-of-participants" class="level4">
<h4 class="anchored" data-anchor-id="number-of-participants">Number of Participants</h4>
<p>We estimated how long it would take a participant to complete our lyrics questionnaire, and the time it would take to complete all questions for a single lyric stimulus on average. Data were collected during our second pilot (see <a href="#sec-pilot2">Section&nbsp;1.2.2</a>).</p>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-2" class="anchored">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;1<strong>.</strong> Time in seconds per lyric stimulus</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">outliers</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">mean</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">median</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">no outliers removed</td>
<td style="text-align: right;">40.34782</td>
<td style="text-align: right;">30.4965</td>
<td style="text-align: right;">43.49806</td>
</tr>
<tr class="even">
<td style="text-align: left;">outliers set at &lt; 900</td>
<td style="text-align: right;">39.94714</td>
<td style="text-align: right;">30.4850</td>
<td style="text-align: right;">35.46503</td>
</tr>
</tbody><tfoot>
<tr class="odd">
<td style="text-align: left; padding: 0;"><span style="font-style: italic;">Note: </span> <sup></sup> Mean, median and standard deviation time in *seconds* per lyric stimulus determined by subtracting time at the first click in a block of questions from the last click for each song.</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tfoot>

</table>
</div>


</div>
</div>
<p>We aimed for a 30-minute survey. We estimated conservatively that it would take approximately 85 seconds to complete each lyric stimulus item, and approximately 3 minutes (240 seconds) to complete the other items in the survey. Thus we had room for 18 lyric stimulus items.</p>
<p>Given the total of 360 lyric stimuli and the time taken per stimulus, we estimated the number of participants necessary to receive approximately 25 ratings per stimuli using simulation <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. We thus expect to collect data for 530 participants. See <a href="#sec-participantsimulation">Section&nbsp;1.6</a> for further details.</p>
</section>
</section>
<section id="natural-language-processing" class="level3">
<h3 class="anchored" data-anchor-id="natural-language-processing">Natural Language Processing</h3>
<section id="lexicon" class="level4">
<h4 class="anchored" data-anchor-id="lexicon">Lexicon</h4>
<p>We will use the <code>Refined_dictionary.txt</code> file, included in the supplementary materials of <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span> stored on the <a href="https://osf.io/vy475/">Open Science Framework</a>.</p>
</section>
<section id="pre-trained-models" class="level4">
<h4 class="anchored" data-anchor-id="pre-trained-models">Pre-trained models</h4>
<p>We consider two commonly used pre-trained word-embedding models: <code>word2vec-google-news</code> is trained on a corpus of online news articles, including about 100 billion words. It is based on the work of <span class="citation" data-cites="mikolov_distributed_2013">Mikolov et al. (<a href="#ref-mikolov_distributed_2013" role="doc-biblioref">2013</a>)</span>. <code>GloVe-common-crawl-840B</code> is trained with the model suggested by <span class="citation" data-cites="pennington_glove_2014">Pennington, Socher, and Manning (<a href="#ref-pennington_glove_2014" role="doc-biblioref">2014</a>)</span> using crawled large-scale web pages, including about 840 billion words.</p>
</section>
<section id="models-we-trained" class="level4">
<h4 class="anchored" data-anchor-id="models-we-trained">Models we trained</h4>
<p>We will also consider word embedding models directly trained from the lyrics corpus, based on <span class="citation" data-cites="pennington_glove_2014">Pennington, Socher, and Manning (<a href="#ref-pennington_glove_2014" role="doc-biblioref">2014</a>)</span>. We select models on two sets of off-line criteria:</p>
<ol type="1">
<li>loss function on the hold-out data set and</li>
<li>English word similarity judgment data employed from <span class="citation" data-cites="faruqui_community_2014">Faruqui and Dyer (<a href="#ref-faruqui_community_2014" role="doc-biblioref">2014</a>)</span>.</li>
</ol>
</section>
<section id="word-vector-aggregation" class="level4">
<h4 class="anchored" data-anchor-id="word-vector-aggregation">Word-vector aggregation</h4>
<p>It is necessary to aggregate word vectors from each lyric into a single lyric vector, which is then compared to sets of words belonging to each value. We consider two methods: 1) uniform average and 2) weighted sum. In particular, weighted sum employs the inverse-document-frequency for weighting each word vector within the lyrics <span class="citation" data-cites="de_boom_representation_2016">De Boom et al. (<a href="#ref-de_boom_representation_2016" role="doc-biblioref">2016</a>)</span>.</p>
</section>
</section>
<section id="analytic-approach" class="level3">
<h3 class="anchored" data-anchor-id="analytic-approach">Analytic approach</h3>
<p>Our primary hypothesis is that we will observe a correlation in ratings between participants and output from NLP systems (see <a href="#sec-hypotheses">Section&nbsp;0.1.2</a> and <a href="#sec-magnitudes">Section&nbsp;1.1</a>).</p>
<p>Estimating a ‘ground truth’ for each lyric stimulus from participant ratings is non-trivial. Specifically, participants will use our survey instrument differently (e.g.&nbsp;some will give overall more extreme scores, whereas some will more consistently give scores close to the middle of the slider bar). We therefore aim to estimate scores for stimuli while statistically controlling for the effect of participant’s tendency to use the survey.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div>
<pre class="mermaid mermaid-js" data-label="fig-4">flowchart RL
  subgraph observed
  B[raw responses]
  end
  subgraph intercepts
  C((Participants)) --&gt; B 
  E((Stimuli)) --&gt; B
  end
  
  style intercepts fill:#8080, stroke:#333, color:#FFFFFF
  style observed fill:#8080, stroke:#333, color:#FFFFFF
</pre>
</div>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Cross-classified model, with intercepts estimated for individual participants and individual lyric stimuli</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We take a cross-classified approach, whereby we attempt to explicitly model the tendency of participant use of the survey by estimating a <code>participant intercept</code> for each one. Our primary analysis involves correlating the <code>stimuli intercept</code><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> with output for automated systems.</p>
<p>Similar to <span class="citation" data-cites="beaty_automating_2021">Beaty and Johnson (<a href="#ref-beaty_automating_2021" role="doc-biblioref">2021</a>)</span>, the ratings from different NLP systems will be linearly combined into a latent variable for each value. These are then correlated to the <code>stimuli intercepts</code>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div>
<pre class="mermaid mermaid-js" data-label="fig-5">flowchart LR
  subgraph intercepts
  C((Participants))
  E((Stimuli))
  end
  subgraph NLP_models
  F((machines)) --&gt; M1
  F((machines)) --&gt; M2
  F((machines)) --&gt; M3
  F((machines)) --&gt; MN
  end
  E&lt;--&gt;F
  style intercepts fill:#8080, stroke:#333, color:#FFFFFF
  style NLP_models fill:#8080, stroke:#333, color:#FFFFFF
</pre>
</div>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> Latent variable representing ratings from multiple NLP Systems</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We will use the proprietary software, <a href="https://www.statmodel.com">Mplus</a> to estimate the models<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="contributions" class="level3">
<h3 class="anchored" data-anchor-id="contributions">Contributions</h3>
<p>We extend existing work primarily in three ways: Firstly, we examine whether the work in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span> can be extended to song lyrics: <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span> showed evidence of validity for a lexicon of words for measuring a set of 10 personal values in social-media text.</p>
<p>Secondly, we compare semantic distance (the degree to which words are related) estimated using NLP systems to the results of word counting. Prior studies have counted the number of times specific words from a fixed lexicon were used in a given body of text as a means of measuring psychological constructs <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span>. However, song lyrics may not contain those exact words, and may instead use synonymous or otherwise meaningfully similar words, or even slang and metaphors. Our method allows for more word coverage: rather than count words from a fixed lexicon, we will estimate the semantic distance between the words in the lexicon that represent each personal value, and the song lyrics in order to derive a score for each value.</p>
<p>Thirdly, we linearly combine the output of multiple NLP systems into a single latent variable, to represent the shared variance of the machine ratings: as each NLP system is developed using 1) an algorithm trained on 2) a corpus, each algorithm/corpus combination will estimate the semantic distance between two words differently. This loosely parallels how human participants may rate each set of lyrics differently. <span class="citation" data-cites="beaty_automating_2021">Beaty and Johnson (<a href="#ref-beaty_automating_2021" role="doc-biblioref">2021</a>)</span> showed that this latent variable of semantic distance estimations resulted in overlap with a latent variable of human ratings as high as r = .9, albeit in a different domain. This approach further allows us to estimate the contribution of each algorithm / corpus setup to the shared variance.</p>
<p>We further contribute three assets: firstly, we provide containerized, API-reachable interface to the models that we used to estimate semantic similarity, housed on <a href="https://replicate.com/eldrin/text-concept-similarity">Replicate</a>. Secondly, we share code notebooks written with the intention of allowing for reproducibility, replication, and extension of our work. Thirdly, we share the beginnings of a psychometric scale for assessing lyric preference intensity and expertise (<a href="#sec-lyricquestions">Section&nbsp;1.3</a>).</p>
</section>
</section>



<div id="quarto-appendix" class="default"><section id="appendix" class="level1 appendix"><h2 class="anchored quarto-appendix-heading">Appendix</h2><div class="quarto-appendix-contents">

<section id="sec-magnitudes" class="level2">
<h2 class="anchored" data-anchor-id="sec-magnitudes">a) hypothesized magnitudes</h2>
<ul>
<li><p>LIWC correlations with participant ratings of Personal Values <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>:</p>
<ul>
<li>liwc insight with self-direction, .43</li>
<li>liwc sexuality with hedonism, .13</li>
<li>liwc achievement with achievement .47</li>
<li>liwc power with power, .19</li>
<li>liwc power with conformity, .16</li>
<li>liwc risk with security, .32</li>
<li>liwc religion with traditionalism, .79</li>
<li>liwc family with benevolence, .57</li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>Correlations of self ratings of Personal Values with automated estimates from essays <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>:</p>
<ul>
<li>self-direction: .23</li>
<li>stimulation: .12</li>
<li>hedonism: .22</li>
<li>achievement: .17</li>
<li>power: -.02</li>
<li>security: .00</li>
<li>conformity: .07</li>
<li>tradition: .31</li>
<li>benevolence: .18</li>
<li>universalism: .29</li>
</ul></li>
</ul>
</section>
<section id="b-number-of-ratings-per-stimulus" class="level2">
<h2 class="anchored" data-anchor-id="b-number-of-ratings-per-stimulus">b) number of ratings per stimulus</h2>
<p>We conducted two pilot studies to estimate the number of ratings needed for each song lyric.</p>
<section id="sec-pilot1" class="level3">
<h3 class="anchored" data-anchor-id="sec-pilot1">Pilot 1</h3>
<p>Our first pilot study aimed to gather an tentative estimate of the time it would take participants to complete components of the survey using a small convenience sample. We recruited participants first on <a href="https://www.reddit.com/r/SampleSize">reddit.com</a> and then from within the lab of the research team. Participants were shown four lyric stimuli and asked to complete our adapted personal values questionnaire for each song lyric. we used the Qualtrics platform to create and host the survey (<a href="https://www.qualtrics.com">qualtrics.com</a>).</p>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-1" class="anchored">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table&nbsp;2<strong>.</strong> Time in minutes to complete task.</caption>
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th">statistic</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">lyric preferences</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">song 1</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">song 2</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">song 3</th>
<th style="text-align: right;" data-quarto-table-cell-role="th">song 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">mean</td>
<td style="text-align: right;">1.4460848</td>
<td style="text-align: right;">2.019053</td>
<td style="text-align: right;">1.626731</td>
<td style="text-align: right;">1.374369</td>
<td style="text-align: right;">1.072906</td>
</tr>
<tr class="even">
<td style="text-align: left;">sd</td>
<td style="text-align: right;">0.7674326</td>
<td style="text-align: right;">1.572428</td>
<td style="text-align: right;">1.742973</td>
<td style="text-align: right;">1.480763</td>
<td style="text-align: right;">1.053221</td>
</tr>
</tbody><tfoot>
<tr class="odd">
<td style="text-align: left; padding: 0;"><span style="font-style: italic;">Note: </span> <sup></sup> Time in minutes determined by subtracting time at the first click in a block of questions from the last click, and dividing by 60.</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tfoot>

</table>
</div>


</div>
</div>
<p>On average participants took 1.52 minutes per song. However, <a href="#tbl-1">Table&nbsp;2</a> shows that the time to complete the items per song decreased as participants progressed through the questionnaires. Thus we estimated that questions for 20 stimuli could be completed in approximately 30 minutes.</p>
</section>
<section id="sec-pilot2" class="level3">
<h3 class="anchored" data-anchor-id="sec-pilot2">Pilot 2</h3>
<p>Following our first pilot (<a href="#sec-pilot1">Section&nbsp;1.2.1</a>), we aimed to estimate the smallest number of ratings necessary to achieve a satisfactory inter-rater reliability. We recruited in proportions for a representative sample of the United States on the <a href="https://www.prolific.co">Prolific.co</a> participant recruitment platform.We gathered responses from 500 participants on 20 lyric stimuli, and the followed a similar procedure as described in <span class="citation" data-cites="debruine_determining_2018">DeBruine and Jones (<a href="#ref-debruine_determining_2018" role="doc-biblioref">2018</a>)</span>. For each of the 10 values, we estimated Cronbach’s alpha for a range of subsample sizes, ranging from 5 to 50 participants in increments of 5. This procedure was repeated 10 times per increment, separately for each of the 10 values. We then examined the distribution of Cronbach’s Alpha for each of the 10 personal values to determine the frequency with which it exceeded a threshold of .7, commonly considered to be an acceptable level of reliability.</p>
<p>The distributions of Cronbach’s Alpha estimates for the value of Stimulation are shown in <a href="#fig-3">Figure&nbsp;5</a>, and suggested the need for more than 25 raters to consistently achieve a Cronbach’s Alpha greater than .7. We thus conclude that a conservative estimate for the number of ratings is 25.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="pre-registration_html_files/figure-html/fig-3-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;5<strong>.</strong> Distribution of Cronbach’s Alpha estimates for the value of Stimulation, by sub-sample size</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-lyricquestions" class="level2">
<h2 class="anchored" data-anchor-id="sec-lyricquestions">c) lyric preference and expertise questionnaire</h2>
<p>This questionnaire remains a work in progress; we include the current version as of this pre-registration.</p>
<ol type="1">
<li>I prefer music that contains lyrics, as opposed to music that does not</li>
<li>I only pay attention to the lyrics of songs or artists that I like</li>
<li>I always pay attention to the lyrics of a song, if the song has them</li>
<li>I enjoy learning about song lyrics and their meaning, for example by reading blogs and forums or listening to artist interviews</li>
<li>If a song has lyrics that I don’t like for any reason, I don’t listen to it</li>
<li>If I am not sure about the lyrics of a song, I look them up</li>
<li>I contribute to online resources on lyrics (e.g.&nbsp;on forums, or on platforms where I can contribute lyric transcriptions)</li>
<li>I memorize the lyrics to the songs I listen to</li>
<li>I write my own song lyrics</li>
<li>I post excerpts of song lyrics online, e.g.&nbsp;on social media</li>
<li>I discuss song lyrics with my friends</li>
<li>I come up with alternate versions of song lyrics that I find entertaining, i.e.&nbsp;song parodies</li>
<li>I ponder the meaning of lyrics</li>
<li>I quote lyrics in conversation</li>
<li>I read and/or write poetry</li>
<li>What percentage of your music library do you think contains songs with lyrics?</li>
</ol>
</section>
<section id="sec-stratifiedsampling" class="level2">
<h2 class="anchored" data-anchor-id="sec-stratifiedsampling">d) stratified sampling for stimulus set</h2>
<p>Our sampling process was as follows:</p>
<ol type="1">
<li>uniformly sub-sample 60k artists out of 300k artists in the MPD</li>
<li>determine song availability on musiXmatch using their API</li>
<li>create a large pool of available songs for the 60k artists</li>
</ol>
<p>We then consider four aspects of lyric data as strata for random sampling:</p>
<ol type="1">
<li>Genre, estimated using topic modeling on artist-tags (Schindler et al., 2012)</li>
<li>Popularity, estimated via artist playlist frequency</li>
<li>Lyric Topic, estimating using topic modeling</li>
<li>Release date</li>
</ol>
<p>Estimated genre and lyric topic resulted in categorical groupings. Popularity and Release date were divided into equally spaced sub-ranges; e.g.&nbsp;we divided release year into decades (60s, 70s, 80s, and so on).</p>
<section id="bias-correction" class="level3">
<h3 class="anchored" data-anchor-id="bias-correction">bias correction</h3>
<p>We expect our dataset will lean towards songs that are a) recent, and b) popular. Specifically, our continuous strata variables are separated into bins, and we expect that some bins in the defined strata will result in very few songs. Thus, we compensate by oversampling the less populated bins. To do so, we employ the maximum-a-posteriori (MAP) estimate of the parameter of the categorical distribution for each stratum: this inflates the probability that songs from the less populated bins will be selected. The procedure is controlled by a free parameter “alpha,” which determines the degree to which we inflate the bins. However, we do not know any prior study that suggests an appropriate alpha that suits our study context. Thus, we heuristically set the parameter to 40,000, which implies that songs in the lesser bins will comprise 5 - 10% of the resulting pool.</p>
<p>The total number of samples was set at 2,200 based on estimations of the research team as to the maximum possible number of songs that could be rated in this study given time and budget constraints.</p>
<p>Further, we select the samples as a dynamic search process rather than a typical sampling procedure:</p>
<ol type="1">
<li>Set “reference distribution” for each stratum, and an empty dataset to populate:
<ol type="a">
<li>each reference distribution is the original distribution compensated by MAP with alpha = 40,000</li>
<li>set the total number of samples (<em>N</em>=2200) to be found</li>
<li>define <em>B</em> as a currently empty set of lyrics which we will populate</li>
</ol></li>
<li>Repeat the following until the number of samples in B reaches N:
<ol type="a">
<li>select a stratum uniformly randomly</li>
<li>select a song such that the distribution of samples in <em>B</em> most closely resembles reference distribution</li>
<li>add song to <em>B</em></li>
</ol></li>
</ol>
<p>From the very first lyric stimulus selected, this procedure will allow for any length of slice of <em>B</em> to approximately follow the reference distribution for each stratum. We expect that this procedure can be useful 1) when the first few items must follow the reference distribution and 2) when there is the possibility of continuing the annotation project at a later time, and thus the sampling procedure must be continued rather than starting anew.</p>
</section>
</section>
<section id="sec-manualscreening" class="level2">
<h2 class="anchored" data-anchor-id="sec-manualscreening">e) manual screening procedure</h2>
<p>Lyrics of the stimulus set were then manually screened to see if they were a match to the actual song <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and for suitability.</p>
<p>Three members of the research team (Sandy Manolios, Jaehun Kim, Andrew M. Demetriou) then examined each set of lyrics, selecting appropriate candidates and resolving disagreements via discussion.</p>
<p>Songs were removed if they were: 1. were not in English 2. completely onomatopoetic 3. repetitions of single words or a single phrase 4. if the three members felt there were too few words</p>
<p>Specifically, researchers examined whether the lyrics were indeed English songs, as our automated screening methods to determine song language are imperfect (e.g.&nbsp;some of the lyrics sets were English translations of the original songs). Selected songs were manually adjusted if the artist name or other additional information was present, or if non-English characters were present <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. Lyrics that did not match the title were also marked, but not changed or excluded. This was coordinated via a shared spreadsheet on google sheets.</p>
</section>
<section id="sec-participantsimulation" class="level2">
<h2 class="anchored" data-anchor-id="sec-participantsimulation">f) simulation for number of participants</h2>
<p>We estimated the number of participants to recruit by conducting a simulation. As we have a pool of 360 lyrics, and aim for approximately 25 ratings each, we must compute how many:</p>
<ol type="1">
<li>song lyrics to show each participant</li>
<li>participants are needed such that each song lyric will receive a median 25 ratings if randomly selected</li>
</ol>
<p>We write a simple program that simulates the survey process;</p>
<ol type="1">
<li>Set parameters for simulation:
<ol type="a">
<li>number of participants (<em>N</em>)</li>
<li>total number of items (<em>M</em>)</li>
<li>number of items to be included in the survey (<em>L</em>)</li>
<li>an empty list where we will add the “seen” items (<em>A</em>)</li>
</ol></li>
<li>Repeat <em>N</em> times:
<ol type="a">
<li>draw <em>L</em> items from the total M items, without replacement // simulating the survey</li>
<li>Add sampled items to <em>A</em> // collecting the seen items</li>
</ol></li>
<li>Output:
<ol type="a">
<li>median number of rated items from empirical distribution of <em>A</em></li>
<li>estimated cost for the campaign using the statistics from the pilot survey</li>
</ol></li>
</ol>
<p>Then we compute this simulation for a range of participants [20, 1000] and lyric stimuli[20, 1000] while keeping the number of stimuli per survey fixed. Based on our second pilot study, we estimated 30 minutes is sufficient time for participants to complete 18 stimuli. Thus our budget limit allows for the rating of 360 items, which we aim to have rated 25 times, in 18-item surveys, from an estimated 530 participants, who spend approximately 30 minutes on task. <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
</section>
<section id="sec-contributions" class="level2">
<h2 class="anchored" data-anchor-id="sec-contributions">g) Team Member Contributions</h2>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-3" class="anchored">
<table class="table table-striped table-sm small" data-quarto-postprocess="true">
<caption>Table&nbsp;3<strong>.</strong> Contributions by Research Team Member</caption>
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th" style="text-align: right; position: sticky; top: 0; background-color: #FFFFFF;">Role</th>
<th data-quarto-table-cell-role="th" style="text-align: center; position: sticky; top: 0; background-color: #FFFFFF;">Andrew*</th>
<th data-quarto-table-cell-role="th" style="text-align: center; position: sticky; top: 0; background-color: #FFFFFF;">Jaehun</th>
<th data-quarto-table-cell-role="th" style="text-align: center; position: sticky; top: 0; background-color: #FFFFFF;">Sandy</th>
<th data-quarto-table-cell-role="th" style="text-align: center; position: sticky; top: 0; background-color: #FFFFFF;">Cynthia</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">conceptualization</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">data curation</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">formal analysis</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">funding acquisition</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">investigation</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">methodology</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">project administration</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">resources</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">software</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">supervision</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">validation</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">visualization</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><span style="     color: black !important;">writing - original draft</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
</tr>
<tr class="even">
<td style="text-align: right;"><span style="     color: black !important;">writing - review and editing</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
<td style="text-align: center;"><span style="     color: black !important;">X</span></td>
<td style="text-align: center;"><span style="     color: red !important;">O</span></td>
</tr>
</tbody><tfoot>
<tr class="odd">
<td style="text-align: right; padding: 0;"><span style="font-style: italic;">Note: </span> <sup></sup> * Corresponding Author</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tfoot>

</table>
</div>


</div>
</div>
<p>Roles determined by the <a href="https://www.kent.ac.uk/guides/credit-contributor-roles-taxonomy#:~:text=CRediT%20(Contributor%20Roles%20Taxonomy)%20is,contribution%20to%20the%20scholarly%20output.">Contributor Roles Taxonomy</a>.</p>

</section>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-beaty_automating_2021" class="csl-entry" role="listitem">
Beaty, Roger E, and Dan R Johnson. 2021. <span>“Automating Creativity Assessment with <span>SemDis</span>: <span>An</span> Open Platform for Computing Semantic Distance.”</span> <em>Behavior Research Methods</em> 53 (2): 757–80. <a href="https://doi.org/10.3758/s13428-020-01453-w">https://doi.org/10.3758/s13428-020-01453-w</a>.
</div>
<div id="ref-boyd_development_2022" class="csl-entry" role="listitem">
Boyd, Ryan L, Ashwini Ashokkumar, Sarah Seraj, and James W Pennebaker. 2022. <span>“The Development and Psychometric Properties of <span>LIWC</span>-22.”</span> <em>Austin, TX: University of Texas at Austin</em>. <a href="https://www.researchgate.net/profile/Ryan-Boyd-8/publication/358725479_The_Development_and_Psychometric_Properties_of_LIWC-22/links/6210f62c4be28e145ca1e60b/The-Development-and-Psychometric-Properties-of-LIWC-22.pdf">https://www.researchgate.net/profile/Ryan-Boyd-8/publication/358725479_The_Development_and_Psychometric_Properties_of_LIWC-22/links/6210f62c4be28e145ca1e60b/The-Development-and-Psychometric-Properties-of-LIWC-22.pdf</a>.
</div>
<div id="ref-cabitza_as_2020" class="csl-entry" role="listitem">
Cabitza, F., A. Campagner, and L. M. Sconfienza. 2020. <span>“As If Sand Were Stone. <span>New</span> Concepts and Metrics to Probe the Ground on Which to Build Trustable <span>AI</span>.”</span> <em>BMC Medical Informatics and Decision Making</em> 20 (1). <a href="https://doi.org/10.1186/s12911-020-01224-9">https://doi.org/10.1186/s12911-020-01224-9</a>.
</div>
<div id="ref-de_boom_representation_2016" class="csl-entry" role="listitem">
De Boom, Cedric, Steven Van Canneyt, Thomas Demeester, and Bart Dhoedt. 2016. <span>“Representation Learning for Very Short Texts Using Weighted Word Embedding Aggregation.”</span> <em>Pattern Recognition Letters</em> 80: 150–56. <a href="https://doi.org/10.1016/j.patrec.2016.06.012">https://doi.org/10.1016/j.patrec.2016.06.012</a>.
</div>
<div id="ref-debruine_determining_2018" class="csl-entry" role="listitem">
DeBruine, LM, and BC Jones. 2018. <span>“Determining the Number of Raters for Reliable Mean Ratings.”</span> <em>Open Science Framework</em>. <a href="https://doi.org/10.17605/OSF">https://doi.org/10.17605/OSF</a>.
</div>
<div id="ref-faruqui_community_2014" class="csl-entry" role="listitem">
Faruqui, Manaal, and Chris Dyer. 2014. <span>“Community Evaluation and Exchange of Word Vectors at Wordvectors. Org.”</span> In <em>Proceedings of 52nd <span>Annual</span> <span>Meeting</span> of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span>: <span>System</span> <span>Demonstrations</span></em>, 19–24. <a href="https://doi.org/10.3115/v1/P14-5004">https://doi.org/10.3115/v1/P14-5004</a>.
</div>
<div id="ref-lindeman_measuring_2005" class="csl-entry" role="listitem">
Lindeman, Marjaana, and Markku Verkasalo. 2005. <span>“Measuring Values with the Short <span>Schwartz</span>’s Value Survey.”</span> <em>Journal of Personality Assessment</em> 85 (2): 170–78. <a href="https://doi.org/10.1207/s15327752jpa8502_09">https://doi.org/10.1207/s15327752jpa8502_09</a>.
</div>
<div id="ref-mikolov_distributed_2013" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. <span>“Distributed <span>Representations</span> of <span>Words</span> and <span>Phrases</span> and Their <span>Compositionality</span>.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, edited by C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger. Vol. 26. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf</a>.
</div>
<div id="ref-pennington_glove_2014" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. <span>“Glove: <span>Global</span> Vectors for Word Representation.”</span> In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (<span>EMNLP</span>)</em>, 1532–43. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-ponizovskiy_development_2020" class="csl-entry" role="listitem">
Ponizovskiy, Vladimir, Murat Ardag, Lusine Grigoryan, Ryan Boyd, Henrik Dobewall, and Peter Holtz. 2020. <span>“Development and Validation of the Personal Values Dictionary: <span>A</span> Theory–Driven Tool for Investigating References to Basic Human Values in Text.”</span> <em>European Journal of Personality</em> 34 (5): 885–902. <a href="https://doi.org/10.1002/per.2294">https://doi.org/10.1002/per.2294</a>.
</div>
<div id="ref-schafer_functions_2009" class="csl-entry" role="listitem">
Schäfer, Thomas, and Peter Sedlmeier. 2009. <span>“From the Functions of Music to Music Preference.”</span> <em>Psychology of Music</em> 37 (3): 279–300. <a href="https://doi.org/10.1177/0305735608097247">https://doi.org/10.1177/0305735608097247</a>.
</div>
<div id="ref-schwartz_extending_2001" class="csl-entry" role="listitem">
Schwartz, Shalom H, Gila Melech, Arielle Lehmann, Steven Burgess, Mari Harris, and Vicki Owens. 2001. <span>“Extending the Cross-Cultural Validity of the Theory of Basic Human Values with a Different Method of Measurement.”</span> <em>Journal of Cross-Cultural Psychology</em> 32 (5): 519–42. <a href="https://doi.org/10.1177/0022022101032005001">https://doi.org/10.1177/0022022101032005001</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>see <code>survey_builder_*.Rmd</code> notebook in the <code>IV_survey_builder</code> folder of this repository.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>see <code>IX_participation_estimation.ipynb</code> notebook in <code>II_rater_pilot</code> folder<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>We refer to this as the “item intercept” in our notebooks. See folder <code>III_simulation_study</code> for further details.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>We built our analysis models using simulated data. We explain the models we will use in the <code>III_simulation_study</code> folder<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>see table 2 in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>see table 4 in <span class="citation" data-cites="ponizovskiy_development_2020">Ponizovskiy et al. (<a href="#ref-ponizovskiy_development_2020" role="doc-biblioref">2020</a>)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>see <code>I_lyric_checker*.Rmd</code> in the <code>IV_survey_builder</code> folder<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>see <code>II_manual_lyric_adjustment*.Rmd</code> in the <code>IV_survey_builder</code> folder for a list of specific changes<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>See <code>IX_participation_estimation.ipynb</code> notebook in <code>II_rater_pilot</code> folder for further details. Though we will not be actively maintaining it, a live version may still be available on this <a href="https://colab.research.google.com/drive/1-gp0lTBTVe0lmHBJEz1K5HHg6dmNpUEE#scrollTo=nMSiUmCX2oEY">collab notebook</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>